# 🧠 Task 5 - Decision Trees and Random Forests



---

### 📁 Dataset Used:
- *Heart Disease Dataset*  
- Columns include age, sex, cp (chest pain), thalach (max heart rate), chol (cholesterol), etc.  
- Target column: target (1 = disease, 0 = no disease)

---

### 🔧 Tools & Libraries:
- Python
- Scikit-learn
- Matplotlib & Seaborn
- Pandas, NumPy

---

### 🧪 Models Trained:
1. *Decision Tree Classifier*
2. *Pruned Decision Tree (max_depth=4)*
3. *Random Forest Classifier (100 trees)*

---

### 📈 Accuracy Results:
| Model                | Accuracy |
|---------------------|----------|
| Decision Tree        | ~accuracy from output |
| Pruned Decision Tree | ~accuracy from output |
| Random Forest        | ~accuracy from output |

---

### 🔬 Key Learnings:
- Decision trees can overfit; pruning helps control complexity.
- Random Forest reduces variance and improves accuracy via ensemble learning.
- Feature importance helps interpret model decisions.
- Cross-validation gives a better measure of generalization.

---

### 📊 Visuals:
- Decision Tree plot  
- Feature importance bar chart

---
